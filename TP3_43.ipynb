{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.integrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thème 3 : Analyse Bayésienne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jeu du pile ou face\n",
    "\n",
    "On souhaite estimer, à partir d'un série de N mesures au jeu de pile ou face, la probabilité $p$ d'obtenir pile.\n",
    "Pour cela, dans le cadre de l'analyse bayésienne on va considérer $p$ comme une variable aléatoire et on va construire sa fonction densité de probabilité à l'aide du théorème de Bayes~(on suppose que l'on a observé n piles parmi les N mesures):\n",
    "\n",
    "$$ f_N(p|n) \\propto P_N(n|p) \\times \\pi (p) $$\n",
    "\n",
    "où $f_N(p|n)$ est la fonction de distribution associée à $p$, $P_N(n|p)$ est la probabilité d'observer $n$ piles\n",
    "parmi $N$ mesures pour un $p$ donné (fonction de vraisemblance) et $\\pi(p)$ est la fonction densité de\n",
    "probabilité associée à $p$ à priori (ce que l'on appelle le prior).\n",
    "\n",
    "1. Donner la forme de $P_N(n|p)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La probabilité d'avoir n piles sachant la valeur de p, est donnée par une loi binomiale.\n",
    "\n",
    " $P_N(n|p) = {n \\choose k} p^k (1-p)^{n-k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Simuler une série de $128$ mesures effectuées avec $p=.8$ et stocker les résultats dans un vecteur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Là on simule PN(n|p) car on a N = 128 lancés, et on connaît p=0.8. \n",
    "\n",
    "vecteur = np.random.random(128)<=0.8\n",
    "n = np.sum(vecteur)\n",
    "print('Nombre de piles =',n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans la suite, on prend comme prior $\\pi (p) = 1$ (toutes les valeurs de $p$ sont équiprobables).\n",
    "\n",
    "3. Représenter $f_{N}(p|n)$ pour N = 1, 2, 4, 8, 16, 32, 64, 128. On prendra soin de normaliser la fonction densité\n",
    "de probabilité (on peut utiliser la méthode `scipy.integrate.simps` pour intégrer numériquement\n",
    "une fonction entre $a$ et $b$). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, figsize=(10,6))\n",
    "\n",
    "for N in [2**i for i in range(8)]:\n",
    "    mesures = np.random.random(128)<=0.8\n",
    "    p = np.linspace(0, 1, 1000)\n",
    "    n = np.sum(mesures[:N])\n",
    "    # J'ai rajouté le coefficient binomial ..\n",
    "    y = scipy.special.binom(N,n)*p**n*(1-p)**(N-n)\n",
    "    norm = scipy.integrate.simps(y, p)\n",
    "    ax1.plot(p, y/norm, label=f'N={N}', lw = 3) \n",
    "    ax1.legend()\n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "ax1.set_xlabel('p', fontsize =18, color = 'b')\n",
    "ax1.set_ylabel('fN(p|n)', fontsize =18, color = 'b')\n",
    "ax1.set_title('Distribution de probabilité de p sachant n et N\\n', fontsize = 18, color='darkorange')\n",
    "plt.show()\n",
    "print(color.PURPLE + color.BOLD +'\\nfN(p|n), c est la probabilité d avoir p, sachant que l on a obtenu '\n",
    "      'n piles pour N lancés.\\n \\n'+ color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Pour chaque cas, en déduire la valeur la plus probable de $p$ : $p_{MV}$ (on peut utiliser la fonction `np.argmax`), comparer à la valeur théorique. Calculer la variance $\\sigma^2$ de $p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. En déduire le niveau de confiance associé à l'intervalle $[ p_{MV}-\\sigma, p_{MV}+\\sigma ]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sig):\n",
    "    return 1./(np.sqrt(2.*np.pi)*sig)*np.exp(-np.power((x - mu)/sig, 2.)/2)\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "for N in [2**i for i in range(8)]:\n",
    "    \n",
    "    mesures = np.random.random(1000)<=0.8\n",
    "    p = np.linspace(0, 1, 10000)\n",
    "    n = np.sum(mesures[:N])\n",
    "    \n",
    "    def f(x) :\n",
    "        return (scipy.special.binom(N,n)*x**n*(1-x)**(N-n))/norm\n",
    "   \n",
    "    x = np.linspace(0, 1, 10000)\n",
    "    y = scipy.special.binom(N,n)*p**n*(1-p)**(N-n)\n",
    "    norm = scipy.integrate.simps(y, p)\n",
    "    popt,pcov = scipy.optimize.curve_fit(gaussian,p, y/norm)\n",
    "    niveau_confiance = scipy.integrate.quad(f, p[np.argmax(y/norm)] - popt[1] , p[np.argmax(y/norm)] + popt[1])\n",
    "    \n",
    "    print(color.DARKCYAN + '\\nNombre de lancés :'+ color.END, N)\n",
    "    print(color.BLUE +'Valeur la plus probable de p :'+ color.END,np.round(p[np.argmax(y/norm)],4))\n",
    "    print(color.GREEN +'Variance de la distribution de p :'+ color.END, np.round(popt[1]**2,4))\n",
    "    print(color.PURPLE +'Ecart à la valeur théorique de p :'+ color.END, np.round(abs(0.8-p[np.argmax(y/norm)]),4))\n",
    "    print(color.BLUE +'niveau de confiance :'+ color.END, np.round(niveau_confiance[0]*100,2),'%' )\n",
    "    print(color.GREEN +'Ecart au niveau de confiance théorique :'+ color.END, np.round(abs(0.682-niveau_confiance[0])*100,2),'%' )\n",
    " \n",
    "print(color.PURPLE + color.BOLD +'\\nPlus N est grand, plus on tend vers pmv = 0.8, et plus la variance diminue.'+ color.END)\n",
    "print(color.PURPLE + color.BOLD +'\\nLe niveau de confiance défini comme l intégrale de la distribution entre les bornes sus-citées, '\n",
    "      'tend vers sa valeur\\ncaractéristique pour une gaussienne, à savoir 68.2%.'+ color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Comment évoluent les  résultats obtenus si on prend un autre prior ? essayer par exemple $\\pi (p) = 2 p$ ou\n",
    " $\\pi (p) = 3 p^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,6))\n",
    "\n",
    "for N in [2**i for i in range(10)]:\n",
    "    r = np.random.random(1000)<=0.8\n",
    "    p = np.linspace(0, 1, 1000)\n",
    "    n = np.sum(r[:N])\n",
    "    y1 = scipy.special.binom(N,n)*p**n*(1-p)**(N-n)\n",
    "    y2 = scipy.special.binom(N,n)*p**n*(1-p)**(N-n)*2*p\n",
    "    y3 = scipy.special.binom(N,n)*p**n*(1-p)**(N-n)*3*p**2\n",
    "    norm1 = scipy.integrate.simps(y1, p)\n",
    "    norm2 = scipy.integrate.simps(y2, p)\n",
    "    norm3 = scipy.integrate.simps(y3, p)\n",
    "    ax1.plot(p, y1/norm1, label=f'N={N}', lw = 3) \n",
    "    ax1.set_xlabel('p', fontsize =18, color = 'b')\n",
    "    ax1.set_ylabel('fN(p|n)', fontsize =18, color = 'cyan')\n",
    "    ax1.set_title('Prior = 1', color = 'r', fontsize =18)\n",
    "    ax1.legend()\n",
    "    ax2.plot(p, y2/norm2, label=f'N={N}', lw = 3)\n",
    "    ax2.set_xlabel('p', fontsize =18, color = 'b')\n",
    "    ax2.set_ylabel('fN(p|n)', fontsize =18, color = 'b')\n",
    "    ax2.set_title('Prior = 2p', color = 'r', fontsize =18)\n",
    "    ax2.legend()\n",
    "    ax3.plot(p, y3/norm3, label=f'N={N}', lw = 3)\n",
    "    ax3.set_xlabel('p', fontsize =18, color = 'b')\n",
    "    ax3.set_ylabel('fN(p|n)', fontsize =18, color = 'purple')\n",
    "    ax3.set_title('Prior = 3p²', color = 'r', fontsize =18)\n",
    "    ax3.legend()\n",
    "    \n",
    "    if N< 20 :\n",
    "        \n",
    "        print('\\nPrior = 1 :')\n",
    "        print(color.DARKCYAN + 'Nombre de lancés :'+ color.END, N)\n",
    "        print(color.BLUE +'Valeur la plus probable de p :'+ color.END,np.round(p[np.argmax(y1/norm1)],4))\n",
    "        print(color.PURPLE +'Ecart à la valeur théorique de p :'+ color.END, np.round(abs(0.8-p[np.argmax(y1/norm1)]),4))\n",
    "\n",
    "        print('\\nPrior = 2p :')\n",
    "        print(color.DARKCYAN + 'Nombre de lancés :'+ color.END, N)\n",
    "        print(color.BLUE +'Valeur la plus probable de p :'+ color.END,np.round(p[np.argmax(y2/norm2)],4))\n",
    "        print(color.PURPLE +'Ecart à la valeur théorique de p :'+ color.END, np.round(abs(0.8-p[np.argmax(y2/norm2)]),4))\n",
    "\n",
    "        print('\\nPrior = 3p² :')\n",
    "        print(color.DARKCYAN + 'Nombre de lancés :'+ color.END, N)\n",
    "        print(color.BLUE +'Valeur la plus probable de p :'+ color.END,np.round(p[np.argmax(y3/norm3)],4))\n",
    "        print(color.PURPLE +'Ecart à la valeur théorique de p :'+ color.END, np.round(abs(0.8-p[np.argmax(y3/norm3)]),4))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(color.PURPLE + color.BOLD +'Plus la puissance de p dans le prior est grande et plus les petits N se rapprochent rapidement de p=0.8. '\n",
    "      '\\nNous pouvons le voir assez clairement sur les 3 graphiques jusqu à N = 4, mais également numériquement \\ngrâce à l écart'\n",
    "      ' à la valeut théorique de p.'+ color.END)\n",
    "print(color.PURPLE + color.BOLD +'\\nAfin de montrer uniquement l influence du prior, les graphiques sont réalisés à partir des mêmes mesures.'+ color.END)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loi exponentielle\n",
    "\n",
    "On souhaite estimer le paramètre $\\lambda$ d'une loi exponentielle de fonction densité de probabilité :\n",
    "\n",
    "$$f(x) = \\left\\{ \\begin{array}{ll} \\lambda \\exp ( -\\lambda x ) & \\textrm{pour } x\\ge 0 \\\\\n",
    "0 & \\textrm{sinon} \\end{array} \\right.$$\n",
    "\n",
    "à partir d'une série de $N$ mesures ${x_1, x_2,\\ldots,x_N}$. Dans le cadre de l'analyse bayésienne, on va\n",
    "construire la fonction densité de probabilité associée au paramètre $\\lambda$ sous la forme~:\n",
    "\n",
    "$$ f(\\lambda | {x_1, x_2,\\ldots,x_N} ) \\propto L({x_1, x_2,\\ldots,x_N}|\\lambda) \\times \\pi (\\lambda) $$\n",
    "\n",
    "où $ L({x_1, x_2,\\ldots,x_N}|\\lambda) $ est la fonction de vraisemblance.\n",
    "\n",
    "1. Donner la forme de $ L({x_1, x_2,\\ldots,x_N}|\\lambda) $ :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " $ L({x_1, x_2,\\ldots,x_N}|\\lambda) = \\prod_{i = 1}^{n} \\lambda \\exp ( -\\lambda xi ) = \\lambda^{n} \\exp ( -\\lambda \\sum_{i = 1}^{n} xi )$\n",
    " \n",
    "Son logarithme népérien vaut donc :\n",
    "\n",
    "$$ \\ln L (\\tau) =  n \\ln \\lambda - \\lambda \\sum_i x_i $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Simuler une série de $128$ mesures effectuées avec $\\lambda=.5$ et stocker le résultat des mesures dans un vecteur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecteur =np.random.exponential(2, 128)\n",
    "\n",
    "print('\\nAttention np.random.exponential prend comme argument 1/Lambda\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la suite, on prend comme prior $\\pi (\\lambda) = \\textrm{cst}$ pour $ 0 \\le \\lambda \\le 3 $\n",
    "\n",
    "\n",
    "3. Représenter $ f(\\lambda | {x_1, x_2,\\ldots,x_N}) $ pour N = 1, 2, 4, 8, 16, 32, 64, 128.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention il faut prendre 2 en argument car 2 = 1/0.5.\n",
    "vecteur = np.random.exponential(2, 128)\n",
    "\n",
    "plt.figure(figsize=[15, 7]) \n",
    "\n",
    "def ln_vraisemblance(Lambda, vecteur):\n",
    "    return len(vecteur) * np.log(Lambda) - np.sum(vecteur) * Lambda\n",
    "\n",
    "def ln_prior(Lambda): \n",
    "    return 0\n",
    "\n",
    "def ln_postérior(Lambda, vecteur):\n",
    "    ln_vraisemblance(Lambda, vecteur) + ln_prior(Lambda)\n",
    "    \n",
    "Lambda = np.linspace(0.001, 3, 10000)\n",
    "\n",
    "\n",
    "for n in [2**i for i in range(8)]:\n",
    "    log_y = ln_vraisemblance(Lambda, vecteur[:n]) + ln_prior (Lambda)\n",
    "    y = np.exp(log_y-np.max(log_y))\n",
    "    norm = scipy.integrate.simps(y, Lambda)\n",
    "    plt.plot(Lambda, y/norm, label=f'N = {n}')\n",
    "    plt.xlabel('Lambda', color='orange', fontsize = 20)\n",
    "    plt.ylabel('f(Lambda|x1 ...xn)', color='orange', fontsize = 20)\n",
    "    plt.title('Distribution de probabilité de Lambda à partir des mesures effectuées\\n', color='b', fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pour chaque cas, en déduire la valeur plus probable de $\\lambda_{MV}$ et sa variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. En déduire le niveau de confiance associé à l'intervalle $[\\lambda_{MV}-\\sigma, \\lambda_{MV}+\\sigma]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "vecteur = np.random.exponential(2, 128)\n",
    "\n",
    "def ln_vraisemblance(Lambda, vecteur):\n",
    "    return len(vecteur) * np.log(Lambda) - np.sum(vecteur) * Lambda\n",
    "\n",
    "#Comme le prior doit être constant, j'ai pris 1.\n",
    "def ln_prior(Lambda): \n",
    "    return 0\n",
    "\n",
    "def ln_postérior(Lambda, vecteur):\n",
    "    ln_vraisemblance(Lambda, vecteur) + ln_prior(Lambda)\n",
    "    \n",
    "Lambda = np.linspace(0.001, 3, 10000)\n",
    "\n",
    "\n",
    "for n in [2**i for i in range(8)]:\n",
    "    log_y = ln_vraisemblance(Lambda, vecteur[:n]) + ln_prior (Lambda)\n",
    "    y = np.exp(log_y-np.max(log_y))\n",
    "    norm = scipy.integrate.simps(y, Lambda)\n",
    "    popt,pcov = scipy.optimize.curve_fit(gaussian,Lambda,y/norm)\n",
    "    \n",
    "    def f(x):\n",
    "        return (np.exp(n*np.log(x) - np.sum(vecteur[:n])*x-np.max(log_y)))/norm\n",
    "    \n",
    "    print(color.BOLD +'\\nPour n ='+ color.END, n)\n",
    "    print(color.BOLD +'Valeur la plus probable de Lambda :'+ color.END,np.round(Lambda[np.argmax(y/norm)],4))\n",
    "    print(color.BOLD +'Ecart à la valeur théorique :'+ color.END, np.round(abs(0.5 -Lambda[np.argmax(y/norm)]),4))\n",
    "    print(color.BOLD +'Variance de la distribution de Lambda :'+ color.END, np.round(popt[1]**2,4))\n",
    "    if n > 1 :\n",
    "        niveau_confiance = scipy.integrate.quad(f, Lambda[np.argmax(y/norm)] - popt[1] , Lambda[np.argmax(y/norm)] + popt[1])\n",
    "        print(color.RED +color.BOLD + 'Niveau de confiance'+ color.END, np.round((niveau_confiance[0]*100),2), '%')\n",
    "\n",
    "print(color.PURPLE +color.BOLD +'\\nOn tend bien vers un niveau de confiance de 68.2%, qui est celui d une gaussienne pour 1 sigma.')\n",
    "print(color.PURPLE +color.BOLD +'Je n ai pas calculé celui pour n=1, car la distribution est parfois tellement éloignée de celle d une gaussienne'\n",
    "     ' qu il y a une erreur dans le calcul de l intégrale.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loi gaussienne\n",
    "\n",
    "On souhaite estimer les paramètres $\\mu$ et $\\sigma$ d'une loi gaussienne de fonction densité de probabilité :\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left( - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "à partir d'une série de $N=1000$ mesures ${x_1, x_2,\\ldots,x_N}$ avec  $\\mu=1$ et $\\sigma=2$ , utiliser la bibliothèque `emcee` (https://emcee.readthedocs.io/) pour estimer les lois de distribution associées aux paramètres $\\mu$ et $\\sigma$. Discuter le choix des priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesures = np.random.normal(1, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_vraisemblance(paramètres, mesures):\n",
    "    return -len(mesures) * np.log(paramètres[1]) - .5 * np.sum(np.square(mesures - paramètres[0])/paramètres[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prior = 1 donc ln(prior) = 0\n",
    "def ln_prior(paramètres):\n",
    "    return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_posterior(paramètres, mesures):\n",
    "    return ln_vraisemblance(paramètres, mesures) + ln_prior(paramètres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "nombre_chaînes = 12\n",
    "#Il faut faire 2 colonnes, une pour mu, une autre pour sigma\n",
    "p0 = np.random.random(size=(nombre_chaînes, 2))+1\n",
    "échantillonneur = emcee.EnsembleSampler(nombre_chaînes, 2, ln_posterior, args=[mesures,])\n",
    "échantillonneur.run_mcmc(p0, 10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "échantillons = échantillonneur.get_chain()\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 8))\n",
    "\n",
    "ax1.plot(échantillons[:, :, 0], alpha=0.4)\n",
    "ax1.set_ylim(0, 2)\n",
    "ax1.set_xlim(0, len(échantillons))\n",
    "ax1.set_ylabel('mu',  fontsize = 18)\n",
    "ax1.yaxis.set_label_coords(-0.08, 0.5)\n",
    "ax1.set_xlabel(\"Nombre de pas\", fontsize = 16)\n",
    "\n",
    "ax2.plot(échantillons[:, :, 1], alpha=0.4)\n",
    "ax2.set_ylim(0, 4)\n",
    "ax2.set_xlim(0, len(échantillons))\n",
    "ax2.set_ylabel('sigma', fontsize = 18)\n",
    "ax2.yaxis.set_label_coords(-0.08, 0.5)\n",
    "ax2.set_xlabel(\"Nombre de pas\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLes lois de distribution associées à mu et sigma sont des constantes égales à celles indiquées pour générer les mesures.')\n",
    "print('La superposition des valeurs prises par les 12 chaînes oscillent dans une barre d erreur de 0.3.')\n",
    "print('Si le prior est une constante sa valeur n a pas d incidence, car elle change juste la normalisation de la densité de probabilité.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
